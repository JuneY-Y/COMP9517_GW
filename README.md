ğŸ“Š Project Overview: Aerial Image Classification Benchmark

This repository presents a comprehensive benchmark study on aerial landscape classification across five model families: traditional ML, CNNs (YOLOv8, ResNet), and Transformers (ViT, Swin V2). The goal is to evaluate accuracy, robustness, and long-tail performance under unified settings.

â¸»

ğŸ‹ï¸ Total Workload Summary

We implemented, trained, and compared 20+ model variants, evaluated over 3 benchmark scenarios:
	â€¢	Baseline Testing on balanced data
	â€¢	Robustness Evaluation under noise, distortion, and compression
	â€¢	Long-Tail Testing under severe class imbalance (IR=50)

âœ… Each experiment includes ablations, accuracy, macro-F1, model size, and FLOPs.

â¸»

ğŸ’¡ Main Experiments Breakdown

1. Traditional Machine Learning (10 classifiers Ã— 6 PCA settings)
	â€¢	Extracted Color-LBP + Color-SIFT
	â€¢	Conducted 414 hyperparameter-PCA pipelines
	â€¢	Benchmarked with perturbation and long-tail variants

2. YOLOv8 Variants (10+ ablation studies)
	â€¢	C2f vs. C3, SPPF vs. SPP, Anchor-Free vs. Anchor-Based
	â€¢	SE/CBAM Attention, ViT-Backbone hybrid
	â€¢	KL Consistency, MixUp, Mosaic, LMA, etc.

3. ResNet Backbone + Classifier Swaps
	â€¢	Compared fc, SVM, MLP, KNN, ProtoNet
	â€¢	Re-sampling for imbalance
	â€¢	Perturbation-aware retraining

4. ViT (Base)
	â€¢	Position Encoding and Transformer Layer Ablation
	â€¢	Two-Stage Training
	â€¢	LMA for perturbation robustness

5. Swin Transformer V2
	â€¢	Fine-tuned with 7 augmentation strategies
	â€¢	Tested StepLR, Full Aug, and Tiny variant
	â€¢	Ablated depth, shift-windows, window size
	â€¢	Best long-tail & robustness performance

---

## ğŸ’¡ Main Experiments Breakdown
For detailed configurations of each model, please refer to the corresponding model folder.
### [ğŸ“ Traditional Machine Learning](./Machine%20Learning/)
- 10 classifiers Ã— 6 PCA settings
- Extracted Color-LBP + Color-SIFT features
- Conducted 414 hyperparameter-PCA pipelines
- Benchmarked with perturbation and long-tail variants

### [ğŸ“ YOLOv8 Variants](./YOLO/)
- C2f vs. C3, SPPF vs. SPP, Anchor-Free vs. Anchor-Based
- SE/CBAM Attention, ViT-Backbone hybrid
- KL Consistency, MixUp, Mosaic, LMA, etc.

### [ğŸ“ ResNet Backbone + Classifier Swaps](./ResNet/)
- Compared fc, SVM, MLP, KNN, ProtoNet
- Re-sampling for imbalance
- Perturbation-aware retraining

### [ğŸ“ ViT (Base)](./VIT%20model/)
- Position Encoding and Transformer Layer Ablation
- Two-Stage Training
- LMA for perturbation robustness

### [ğŸ“ Swin Transformer V2](./Swin-Transformer/)
- Fine-tuned with 7 augmentation strategies
- Tested StepLR, Full Aug, and Tiny variant
- Ablated depth, shift-windows, window size
- Best long-tail & robustness performance

---


## ğŸ§µ Key Outcomes

| Model                        | Top-1 Acc | Macro-F1 | Robust Î”L1â€“L3 | Tail-Aware |
|-----------------------------|-----------|----------|----------------|-------------|
| SwinV2 + Dynamic ASL        | 99.1%     | 0.991    | â†“ 6.6%         | âœ… Best      |
| ViT + Two-Stage + LMA       | 97.6%     | 0.976    | â†“ 7.1%         | âœ…           |
| YOLOv8n + CBAM              | 98.0%     | 0.980    | â†“ 26.6%        | â­ Moderate  |
| ResNet-50 + SVM             | 96.1%     | 0.961    | â†“ 28.8%        | âœ…           |
| Traditional Voting          | 86.9%     | 0.869    | â†“ 12.9%        | â­ Basic     |

---




### ğŸ“Š Long-Tail Performance After Mitigation

| Model & Strategy                        | Overall Top-1 (%) | Macro-F1 |
|----------------------------------------|--------------------|----------|
| Balanced Random Forest                 | 73.50              | 0.736    |
| ResNet-50 + SVM + CA-RS                | 90.83              | 0.910    |
| YOLOv8 + KL consistency                | 70.26              | 0.702    |
| ViT-Base + Two-Stage                   | 94.83              | 0.946    |
| **Swin V2 + Dynamic ASL**              | **98.67**          | **0.987** |

---

### ğŸ§ª Robustness After Perturbation-Resilience Optimization

| Model & Strategy                     | Level 1 (light) | Level 2 (medium) | Level 3 (heavy) | Î” (L1â€“L3) |
|-------------------------------------|-----------------|------------------|------------------|-----------|
| Voting + levelâ€‘matched retrain      | 84.00           | 78.17            | 71.06            | 12.94     |
| ResNetâ€‘50 + SVM + perturb           | 95.08           | 87.71            | 66.24            | 28.84     |
| YOLOv8 + KL consistency             | 96.83           | 89.47            | 70.26            | 26.57     |
| **Swin V2 + LMA**                   | **97.69**       | **97.06**        | **91.13**        | **6.56**  |
| ViTâ€‘Base + LMA                      | 96.72           | 95.44            | 89.61            | 7.11      |

---
ğŸ“… Timeline Overview
	â€¢	âœï¸ 20+ models trained (CNNs, ViTs, Swin, ML classifiers)
	â€¢	âš–ï¸ 100+ hours of tuning (aug, loss, optimizer, layers)
	â€¢	ğŸ“Š ~50 ablation experiments across 3 benchmarks
	â€¢	ğŸª¡ 6 key robustness and long-tail strategies tested

â¸»

ğŸš€ Conclusion

This benchmark provides a deep and practical comparison of models, methods, and mitigation strategies in aerial image classification. We hope this unified repo aids future research in robust, fair, and efficient remote sensing AI.
