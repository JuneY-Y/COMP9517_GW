ğŸ“Š Project Overview: Aerial Image Classification Benchmark

This repository presents a comprehensive benchmark study on aerial landscape classification across five model families: traditional ML, CNNs (YOLOv8, ResNet), and Transformers (ViT, Swin V2). The goal is to evaluate accuracy, robustness, and long-tail performance under unified settings.

â¸»

ğŸ‹ï¸ Total Workload Summary

We implemented, trained, and compared 20+ model variants, evaluated over 3 benchmark scenarios:
	â€¢	Baseline Testing on balanced data
	â€¢	Robustness Evaluation under noise, distortion, and compression
	â€¢	Long-Tail Testing under severe class imbalance (IR=50)

âœ… Each experiment includes ablations, accuracy, macro-F1, model size, and FLOPs.

â¸»

ğŸ’¡ Main Experiments Breakdown

1. Traditional Machine Learning (10 classifiers Ã— 6 PCA settings)
	â€¢	Extracted Color-LBP + Color-SIFT
	â€¢	Conducted 414 hyperparameter-PCA pipelines
	â€¢	Benchmarked with perturbation and long-tail variants

2. YOLOv8 Variants (10+ ablation studies)
	â€¢	C2f vs. C3, SPPF vs. SPP, Anchor-Free vs. Anchor-Based
	â€¢	SE/CBAM Attention, ViT-Backbone hybrid
	â€¢	KL Consistency, MixUp, Mosaic, LMA, etc.

3. ResNet Backbone + Classifier Swaps
	â€¢	Compared fc, SVM, MLP, KNN, ProtoNet
	â€¢	Re-sampling for imbalance
	â€¢	Perturbation-aware retraining

4. ViT (Base)
	â€¢	Position Encoding and Transformer Layer Ablation
	â€¢	Two-Stage Training
	â€¢	LMA for perturbation robustness

5. Swin Transformer V2
	â€¢	Fine-tuned with 7 augmentation strategies
	â€¢	Tested StepLR, Full Aug, and Tiny variant
	â€¢	Ablated depth, shift-windows, window size
	â€¢	Best long-tail & robustness performance

â¸»

ğŸ§µ Key Outcomes

Model	Top-1 Acc	Macro-F1	Robust Î”L1-L3	Tail-Aware
SwinV2 + Dynamic ASL	99.1%	0.991	â†“ 6.6%	âœ… Best
ViT + Two-Stage + LMA	97.6%	0.976	â†“ 7.1%	âœ…
YOLOv8n + CBAM	98.0%	0.980	â†“ 26.6%	â­ Moderate
ResNet-50 + SVM	96.1%	0.961	â†“ 28.8%	âœ…
Traditional Voting	86.9%	0.869	â†“ 12.9%	â­ Basic



â¸»

ğŸ“… Timeline Overview
	â€¢	âœï¸ 20+ models trained (CNNs, ViTs, Swin, ML classifiers)
	â€¢	âš–ï¸ 100+ hours of tuning (aug, loss, optimizer, layers)
	â€¢	ğŸ“Š ~50 ablation experiments across 3 benchmarks
	â€¢	ğŸª¡ 6 key robustness and long-tail strategies tested

â¸»

ğŸš€ Conclusion

This benchmark provides a deep and practical comparison of models, methods, and mitigation strategies in aerial image classification. We hope this unified repo aids future research in robust, fair, and efficient remote sensing AI.
