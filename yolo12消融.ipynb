{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0246931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Evaluating model WITH attention:\n",
      "Baseline Accuracy: 90.44%\n",
      "\n",
      "Evaluating model with 6th attention disabled:\n",
      "Disabling attention in module: model.6.m.0.0.attn\n",
      "Disabling attention in module: model.6.m.0.1.attn\n",
      "Disabling attention in module: model.6.m.1.0.attn\n",
      "Disabling attention in module: model.6.m.1.1.attn\n",
      "Accuracy with 6th attention disabled: 84.33%\n",
      "\n",
      "Evaluating model with 8th attention disabled:\n",
      "Disabling attention in module: model.8.m.0.0.attn\n",
      "Disabling attention in module: model.8.m.0.1.attn\n",
      "Disabling attention in module: model.8.m.1.0.attn\n",
      "Disabling attention in module: model.8.m.1.1.attn\n",
      "Accuracy with 8th attention disabled: 88.22%\n",
      "\n",
      "Evaluating model with both 6th and 8th attention disabled:\n",
      "Disabling attention in module: model.6.m.0.0.attn\n",
      "Disabling attention in module: model.6.m.0.1.attn\n",
      "Disabling attention in module: model.6.m.1.0.attn\n",
      "Disabling attention in module: model.6.m.1.1.attn\n",
      "Disabling attention in module: model.8.m.0.0.attn\n",
      "Disabling attention in module: model.8.m.0.1.attn\n",
      "Disabling attention in module: model.8.m.1.0.attn\n",
      "Disabling attention in module: model.8.m.1.1.attn\n",
      "Accuracy with both attention disabled: 78.50%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# ---------------------------\n",
    "# 配置部分\n",
    "# ---------------------------\n",
    "yolo_path = \"/Users/yaogunzhishen/Desktop/best.pt\"  # 预训练 YOLO 分类模型权重路径\n",
    "test_folder = \"/Users/yaogunzhishen/Desktop/datasets/test\"   # 测试集目录，每个类别一个子文件夹\n",
    "\n",
    "# 选择设备（例如 MPS 或 CUDA）\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---------------------------\n",
    "# 辅助函数：加载模型包装器\n",
    "# ---------------------------\n",
    "def load_model_wrapper():\n",
    "    \"\"\"\n",
    "    重新加载一个新的 YOLO 模型包装器\n",
    "    \"\"\"\n",
    "    wrapper = YOLO(yolo_path)\n",
    "    wrapper.model.to(device)\n",
    "    wrapper.model.eval()\n",
    "    return wrapper\n",
    "\n",
    "# ---------------------------\n",
    "# 辅助函数：遍历模型中的注意力模块（AAttn）\n",
    "# ---------------------------\n",
    "def get_attention_modules(model):\n",
    "    \"\"\"\n",
    "    遍历模型中所有子模块，返回所有类型为 AAttn 的模块列表，\n",
    "    每个元素为 (name, module)。\n",
    "    \"\"\"\n",
    "    attn_modules = []\n",
    "    for name, module in model.named_modules():\n",
    "        if module.__class__.__name__ == \"AAttn\":\n",
    "            attn_modules.append((name, module))\n",
    "    return attn_modules\n",
    "\n",
    "# ---------------------------\n",
    "# 辅助函数：禁用（消融）指定注意力模块\n",
    "# ---------------------------\n",
    "def disable_module(module):\n",
    "    \"\"\"\n",
    "    将模块中用于计算注意力的权重置零。\n",
    "    对于 AAttn 模块，尝试将 qkv、proj 和 pe 卷积层的权重置零（如果存在）。\n",
    "    \"\"\"\n",
    "    if hasattr(module, 'qkv') and hasattr(module.qkv, 'weight'):\n",
    "        module.qkv.weight.data.zero_()\n",
    "    if hasattr(module, 'proj') and hasattr(module.proj, 'weight'):\n",
    "        module.proj.weight.data.zero_()\n",
    "    if hasattr(module, 'pe') and hasattr(module.pe, 'conv') and hasattr(module.pe.conv, 'weight'):\n",
    "        module.pe.conv.weight.data.zero_()\n",
    "\n",
    "# ---------------------------\n",
    "# 辅助函数：评估模型在测试集上的准确率\n",
    "# ---------------------------\n",
    "def evaluate_model(model_wrapper, dataset):\n",
    "    \"\"\"\n",
    "    遍历测试集样本，通过文件路径调用模型包装器进行预测，并统计准确率\n",
    "    \"\"\"\n",
    "    num_total = 0\n",
    "    num_correct = 0\n",
    "    for image_path, true_label in dataset.samples:\n",
    "        # 禁用 verbose 输出\n",
    "        model_wrapper.overrides['verbose'] = False\n",
    "        results = model_wrapper(image_path)\n",
    "        if not results:\n",
    "            print(f\"No result for {image_path}\")\n",
    "            continue\n",
    "        res = results[0]\n",
    "        if hasattr(res, \"probs\") and res.probs is not None:\n",
    "            pred_label = int(res.probs.top1)\n",
    "        else:\n",
    "            print(f\"Missing 'probs' attribute for {image_path}\")\n",
    "            continue\n",
    "\n",
    "        num_total += 1\n",
    "        if pred_label == true_label:\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / num_total if num_total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# ---------------------------\n",
    "# 主程序：分阶段消融实验\n",
    "# ---------------------------\n",
    "def main():\n",
    "    # 加载测试集\n",
    "    test_dataset = ImageFolder(root=test_folder)\n",
    "    \n",
    "    # Baseline：带注意力的模型\n",
    "    print(\"Evaluating model WITH attention:\")\n",
    "    wrapper_baseline = load_model_wrapper()\n",
    "    baseline_acc = evaluate_model(wrapper_baseline, test_dataset)\n",
    "    print(\"Baseline Accuracy: {:.2%}\".format(baseline_acc))\n",
    "    \n",
    "    # 消融实验 1：仅禁用第6层注意力（遍历所有注意力模块时下标为5）\n",
    "    print(\"\\nEvaluating model with 6th attention disabled:\")\n",
    "    wrapper_6 = load_model_wrapper()\n",
    "    attn_modules = get_attention_modules(wrapper_6.model)\n",
    "    if len(attn_modules) >= 6:\n",
    "        for i in range(4):\n",
    "            name6, module6 = attn_modules[i]\n",
    "            print(f\"Disabling attention in module: {name6}\")\n",
    "            disable_module(module6)\n",
    "    else:\n",
    "        print(\"Not enough attention modules for 6th layer!\")\n",
    "    acc_6 = evaluate_model(wrapper_6, test_dataset)\n",
    "    print(\"Accuracy with 6th attention disabled: {:.2%}\".format(acc_6))\n",
    "    \n",
    "    # 消融实验 2：仅禁用第8层注意力（下标7）\n",
    "    print(\"\\nEvaluating model with 8th attention disabled:\")\n",
    "    wrapper_8 = load_model_wrapper()\n",
    "    attn_modules = get_attention_modules(wrapper_8.model)\n",
    "    if len(attn_modules) >= 8:\n",
    "        for i in range(4,8):\n",
    "            name6, module6 = attn_modules[i]\n",
    "            print(f\"Disabling attention in module: {name6}\")\n",
    "            disable_module(module6)\n",
    "    else:\n",
    "        print(\"Not enough attention modules for 8th layer!\")\n",
    "    acc_8 = evaluate_model(wrapper_8, test_dataset)\n",
    "    print(\"Accuracy with 8th attention disabled: {:.2%}\".format(acc_8))\n",
    "    \n",
    "    # 消融实验 3：同时禁用第6层和第8层注意力\n",
    "    print(\"\\nEvaluating model with both 6th and 8th attention disabled:\")\n",
    "    wrapper_both = load_model_wrapper()\n",
    "    attn_modules = get_attention_modules(wrapper_both.model)\n",
    "    if len(attn_modules) >= 8:\n",
    "        for i in range(8):\n",
    "            name6, module6 = attn_modules[i]\n",
    "            print(f\"Disabling attention in module: {name6}\")\n",
    "            disable_module(module6)\n",
    "    else:\n",
    "        print(\"Not enough attention modules for both layers!\")\n",
    "    acc_both = evaluate_model(wrapper_both, test_dataset)\n",
    "    print(\"Accuracy with both attention disabled: {:.2%}\".format(acc_both))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
